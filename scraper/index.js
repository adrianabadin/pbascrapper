require('dotenv').config();
const { fetchListingPage, fetchDetalle, fetchTextoActualizado, delay, DELAY_MS } = require('./crawler');
const { parseListingPage, parseDetallePage, parseTextoActualizado } = require('./parser');
const { pool, upsertNormaBasica, upsertNormaDetalle, upsertTextoActualizado, upsertRelaciones, inferirIdentidad } = require('./db');

// Parsear argumentos CLI
const args = process.argv.slice(2);
const tiposArg = args.includes('--tipo')
  ? [args[args.indexOf('--tipo') + 1]]
  : ['ley', 'decreto'];
const desdeAnio = args.includes('--desde')
  ? parseInt(args[args.indexOf('--desde') + 1])
  : null;
const soloListing = args.includes('--solo-listing');
const maxPaginas = args.includes('--max-paginas')
  ? parseInt(args[args.indexOf('--max-paginas') + 1])
  : null;

let erroresConsecutivos = 0;
const MAX_ERRORES_CONSECUTIVOS = 10;

async function procesarNorma(normaBasica) {
  try {
    // 1. Upsert b√°sico (desde listing)
    const { id: normaId } = await upsertNormaBasica(normaBasica);

    if (soloListing) {
      erroresConsecutivos = 0;
      return;
    }

    // 2. Scrape p√°gina de detalle
    await delay(DELAY_MS);
    const detalleHtml = await fetchDetalle(normaBasica.url_canonica);
    const detalle = parseDetallePage(detalleHtml, normaBasica.url_canonica);
    const { sitio_id } = inferirIdentidad(normaBasica.url_canonica);
    await upsertNormaDetalle(sitio_id, detalle);

    // 3. Scrape texto actualizado (si existe)
    if (detalle.url_texto_actualizado) {
      await delay(DELAY_MS);
      const textoHtml = await fetchTextoActualizado(detalle.url_texto_actualizado);
      const articulos = parseTextoActualizado(textoHtml);
      const cambio = await upsertTextoActualizado(normaId, textoHtml, articulos);
      if (cambio) {
        console.log(`üìù ${articulos.length} art√≠culos`);
      } else {
        console.log(`‚úì sin cambios`);
      }
    } else {
      console.log(`- sin texto`);
    }

    // 4. Relaciones normativas
    if (detalle.relaciones.length > 0) {
      await upsertRelaciones(normaId, detalle.relaciones);
    }

    erroresConsecutivos = 0; // reset on success
  } catch (err) {
    erroresConsecutivos++;
    console.error(`\n  ‚ùå Error en ${normaBasica.url_canonica}: ${err.message}`);
    if (erroresConsecutivos >= MAX_ERRORES_CONSECUTIVOS) {
      throw new Error(`Abortando: ${MAX_ERRORES_CONSECUTIVOS} errores consecutivos. √öltimo error: ${err.message}`);
    }
  }
}

async function scrapearTipo(tipo) {
  console.log(`\nüîç Scrapeando ${tipo.toUpperCase()}...`);

  const { html: html1, totalResultados, totalPaginas } = await fetchListingPage(tipo, 1);
  const paginaMaxima = maxPaginas ? Math.min(maxPaginas, totalPaginas) : totalPaginas;
  console.log(`   Total: ${totalResultados} normas, ${paginaMaxima} p√°ginas a procesar`);

  let finalizarTipo = false;

  // Helper to process a page's normas, returns true if should stop
  async function procesarPagina(normas) {
    for (const norma of normas) {
      if (desdeAnio) {
        const { anio } = inferirIdentidad(norma.url_canonica);
        if (anio < desdeAnio) {
          console.log(`\n   ‚èπ Llegamos a ${anio} < ${desdeAnio}, deteniendo.`);
          return true; // signal to stop
        }
      }
      process.stdout.write(`  ‚Üí ${norma.titulo}... `);
      await procesarNorma(norma);
    }
    return false;
  }

  // Process page 1
  const normas1 = parseListingPage(html1);
  finalizarTipo = await procesarPagina(normas1);

  // Process remaining pages
  for (let pagina = 2; pagina <= paginaMaxima && !finalizarTipo; pagina++) {
    console.log(`\nüìÑ P√°gina ${pagina}/${paginaMaxima}...`);
    await delay(DELAY_MS);

    const { html } = await fetchListingPage(tipo, pagina);
    const normas = parseListingPage(html);
    finalizarTipo = await procesarPagina(normas);
  }

  console.log(`\n‚úÖ ${tipo.toUpperCase()} completado`);
}

async function main() {
  console.log('üöÄ Normas GBA Scraper');
  console.log(`   Tipos: ${tiposArg.join(', ')}`);
  if (desdeAnio) console.log(`   Desde a√±o: ${desdeAnio}`);
  if (maxPaginas) console.log(`   M√°x p√°ginas: ${maxPaginas}`);
  if (soloListing) console.log(`   Modo: solo listing (sin detalle)`);

  for (const tipo of tiposArg) {
    await scrapearTipo(tipo);
  }

  await pool.end();
  console.log('\nüéâ Scraping completado');
}

main().catch(async (e) => { console.error('FATAL:', e.message); await pool.end(); process.exit(1); });
